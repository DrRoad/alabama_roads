---
title: "Alabama Roads Project"
authors: "Alexander Hainen and Erik Johnson"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(rgdal)
library(streetview) # devtools::install_github('erikbjohn/streetview')
library(api.keys) # devtools::install_github('erikbjohn/api.keys')
library(ggplot2)
library(geosphere)
library(rgeos)
library(ggmap)
proj.env <- '+proj=longlat +datum=WGS84'
pkg.data.location <- '~/Dropbox/pkg.data/alabama_roads/'
roads.raw.location <- paste0(pkg.data.location, 'raw/us82erik/us82_dissolve')
roads.clean.location <- paste0(pkg.data.location, 'clean/roads.rds')
```

# Background

This is a 'proof of concept' exercise that will attempt the predict road safety using the Google streetview api and a retrained (transfer learning based) version of Tensorflow to enable mass prediction of road safety on large datasets. The ultimate output would be a heat map/scoring of predicted road saftey across Alabama. 


##  Take streetview photos

### Import roads

To do this we will use the road shapefile in the raw project data folder \newline
`r roads.raw.location`. If you do not currently have access to the folder, you can download the folder by going to this [Dropbox web link](https://www.dropbox.com/sh/b7hkvg1xn4nvf87/AADjCfnu1eltuUKyX5HTWvl7a?dl=1). Ideally, you should be able to dynamically link locally by using Dropbox desktop...

To build the streetview folders, the first step is to discretize the road shapefile and then use the google streetview metadata api to get locations of nearest google streetview picture (henceforth called pano_id). In this case the picture will be manually tuned to have the car driving in the 'correct' direction. In the future, this can easily be done with tensorflow as well, by simply ensuring that the photo taken is on the right (not the left) side of the road. Again, this can be easily automated in tensorflow in the future for bigger data applications.


```{r roads_import}
if(!(file.exists(roads.clean.location))){
  roads <- rgdal::readOGR(dsn = path.expand(roads.raw.location), layer = 'us82d', verbose = FALSE)
  roads <- sp::spTransform(roads, CRSobj =CRS(proj.env))
  
  saveRDS(roads, file = roads.clean.location)
} else {
  roads <- readRDS(roads.clean.location)
}
# Plot
map.center <- as.numeric(geosphere::centroid(rgeos::gBuffer(roads, width=0.1)))
map <- ggmap::get_googlemap(center=c(lon=map.center[1], lat=map.center[2]), zoom=7)
roads_fortify <- ggplot2::fortify(roads)
p <- ggmap(map)  +
  geom_line(data=roads_fortify, aes(x=long, y=lat, group=group))
print(p)
```

### Roads Discretize

This creates the points to find the panoids

```{r roads_discretize}
sample.line <- function(x, sdist=100){
    if (!require(sp)) stop("sp PACKAGE MISSING")
     if (!inherits(x, "SpatialLinesDataFrame")) stop("MUST BE SP SpatialLinesDataFrame OBJECT")
      lgth <- SpatialLinesLengths(x) 
      lsub <- x[1,]
        ns <- round( (lgth[1] / sdist), digits=0)
          lsamp <- spsample(lsub, n=ns, type="regular", offset=c(0.5,0.5))
          results <- SpatialPointsDataFrame(lsamp, data=data.frame(ID=rep(1:length(lsamp)))) 
  ( results )
}
road.lines <- sample.line(roads, sdist=0.001)
```

### Pano_ids from meta data


### Pano_ids and camera direction


### Snap street view picture


### Tensorflow